<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Articles on 云原生·盐酒猿</title>
    <link>http://open-native.com/article/</link>
    <description>Recent content in Articles on 云原生·盐酒猿</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 02 Feb 2023 23:05:46 +0800</lastBuildDate><atom:link href="http://open-native.com/article/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>关于 istio-proxy 503/504等5xx问题排查</title>
      <link>http://open-native.com/article/istio-proxy-503/</link>
      <pubDate>Thu, 02 Feb 2023 23:05:46 +0800</pubDate>
      
      <guid>http://open-native.com/article/istio-proxy-503/</guid>
      <description>问题描述 在生产环境，我们最近部署了Istio Service Mesh，Istio控制平面会在每个服务Pod里自动注入一个sidecar。当各个服务都初始化istio-proxy，通过sidecar去实现服务间的调用时，应用和服务就会面临一个很普遍的问题：upstream 服务调用收到HTTP 503/504 response，这个报错信息通常都是由istio-proxy产生的。HTTP 503通常发生在应用和istio-proxy的inbound或者outbound调用，或者是服务网格外部的服务调用。影响面相对严重的是通过网格访问外部服务的outbound流量，相对小的是网格内部的inbound流量。HTTP 504的问题相对要少一些，只有在upstream service的接口response时间超过15秒时才会发生，因为istio-proxy的默认response超时时间就是15秒。 这个问题的现象跟真实的upstream组件失败错误很相似，然而，通过现象分析也有可能是istio本身的默认配置对于网格内部的应用不是那么健壮而问题引起的，对于那些影响面比较大的outbound服务调用：在某一个时间周期内，会有大量的503发生（通常访问量越大报错越多），过一段时间后自己会自愈，周而复始。
影响小一些的inbound调用，在应用监控面板上看不到报错信息，只有在istio-proxy的log里会发现一些503的痕迹，因为我们通常会在客户端或者上游调用时配置重试规则，大部分异常调用都会在重试后成功拿到数据。
对于outbound调用，504 response timeout错误只会对组件依赖超过15秒的请求造成影响。 可以观察一下istio-proxy日志里的HTTP 503/504	upstream错误，进一步的关于日志格式和response flags的信息可以点击链接查看。envoy log
在详细探究这个错误的具体原因之前，我们先看一些错误一日信息的示例： Inbound Call Error example
下面是一个istio-proxy日志里到达请求的response flag “UC”异常日志，含义是：“Upstream connection termination in addition to 503 response code”，由于某种原因在同一个pod里的istio-proxy和他的upstream容器之间的链接被断开了。
还有另外一种response flag “UF”异常日志， 含义是：“Upstream connection failure in addition to 503 response code”，由于某种原因使同一个pod内的istio-proxy和他的upstream容器没办法建立链接
[2022-04-18T09:29:59.633Z] &amp;#34;GET /service_a/v2.1/get?ids=123456 HTTP/1.1&amp;#34; 503 UC &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; 0 95 11 - &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;bc6f9ba3-d8dc-4955-a092-64617d9277c8&amp;#34; &amp;#34;srv-a.xyz&amp;#34; &amp;#34;127.0.0.1:8080&amp;#34; inbound|80|http|srv-a.xyz.svc.cluster.local - 100.64.121.11:8080 100.64.254.9:35174 - [2022-04-18T08:38:43.</description>
    </item>
    
    <item>
      <title>分布式系统中亚稳定失败状态</title>
      <link>http://open-native.com/article/metastable-failure/</link>
      <pubDate>Mon, 30 Jan 2023 14:50:17 +0800</pubDate>
      
      <guid>http://open-native.com/article/metastable-failure/</guid>
      <description>这篇文章的初衷，是记录拜读由Nathan Bronson, Aleksey Charapko, Abutalib Aghayev, and Timothy Zhu共同发表的论文Metastable Failures in Distributed Systems的收获，这篇论文描述了一个在大规模分布式系统中很常见的失败场景：亚稳定失败（metastable failures），它们为什么通常在高负载分布式系统中发生，以及解决问题的思路框架：如何识别和从亚稳定失败中恢复，甚至如何避免发生亚稳定失败。
现实世界中的亚稳定失败 下图是某个公园中非常著名的徒步路线中非常关键的一部分：两座山之间一段狭长的山脊，在两座山之间徒步，只有扶着铁链穿过这段山梁才能保证安全。可以假想这段铁链就是一个分布式系统。 当公园不对徒步者人数做限制时，就有可能引起人群拥挤以至于长时间在起点等待去尝试减低负载。你可以想象一下”系统“经历了以下的状态转换。
稳定状态（Stable state） 当人群低于某个安全阈值时，任何一个风险因素（例如：缓慢通过铁链的徒步者）都可能引起降速，但是系统仍然能够自愈。
脆弱状态（Vulnerable state） 拥挤人群的数量持续增加超过了某个阈值，对于每段铁链的竞争也会增加 —— 下山的人必须要等待上山的人通过，或者干脆冒险不使用铁链而绕过他们。同样，上山的人也必须等待下山的人通过。当这种模式发生时，只要拥挤人群的数量低于某个阈值，系统仍然是可以工作的，但它现在是非常脆弱的，可能变成不可自愈的失败状态。
亚稳定失败状态（Metastable Failure State） 当系统处于脆弱状态，某些风险因素可能导致情况恶化。想象一下，一组徒步者需要花费更长的时间通过铁链，这将会使其他上山和下山的人速度降下来。越来越多的人等待，导致在铁链两端和身在其中的剩余空间越来越狭窄，进而导致人们需要更多的时间通过，进一步导致更多的人等待&amp;hellip;情况循环恶化 —— 进入亚稳定失败状态。 保持系统处于亚稳定失败状态的正反馈循环
由于这个自续的反馈循环，系统将保持在这个状态，即使移除最初的诱发风险因素。为了恢复，需要采取其他的措施，例如将负载降低到特定阈值以下。
亚稳定失败定义 关于亚稳定失败的定义论文中描述如下
亚稳定失败发生在对于负载来源没有控制的开放系统中，一个风险因素导致系统进入一个糟糕的状态，并且会持续存在甚至风险因素被移除。在这个状态系统的效率通常会很低，并且会产生持续影响——通常使工作负载放大或者整体效率降低——阻止系统从这个糟糕的状态中恢复。 Metastable failures occur in open systems with an uncontrolled source of load where a trigger causes the system to enter a bad state that persists even when the trigger is removed. In this state the goodput (i.</description>
    </item>
    
  </channel>
</rss>
